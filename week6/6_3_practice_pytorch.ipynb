{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"6_3_practice_pytorch.ipynb","provenance":[{"file_id":"https://github.com/yandexdataschool/Practical_RL/blob/coursera/week6_outro/seq2seq/practice_pytorch.ipynb","timestamp":1628679651690}],"collapsed_sections":["i73M5XQJ3qt6"]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"aUzB01Th3qtu"},"source":["# Reinforcement Learning for seq2seq\n","\n","This time we'll solve a problem of transribing hebrew words in english, also known as g2p (grapheme2phoneme)\n","\n"," * word (sequence of letters in source language) -> translation (sequence of letters in target language)\n","\n","Unlike what most deep learning practicioners do, we won't only train it to maximize likelihood of correct translation, but also employ reinforcement learning to actually teach it to translate with as few errors as possible.\n","\n","\n","### About the task\n","\n","One notable property of Hebrew is that it's consonant language. That is, there are no vowels in the written language. One could represent vowels with diacritics above consonants, but you don't expect people to do that in everyay life.\n","\n","Therefore, some hebrew characters will correspond to several english letters and others --- to none, so we should use encoder-decoder architecture to figure that out.\n","\n","![img](https://esciencegroup.files.wordpress.com/2016/03/seq2seq.jpg)\n","_(img: esciencegroup.files.wordpress.com)_\n","\n","Encoder-decoder architectures are about converting anything to anything, including\n"," * Machine translation and spoken dialogue systems\n"," * [Image captioning](http://mscoco.org/dataset/#captions-challenge2015) and [image2latex](https://htmlpreview.github.io/?https://github.com/openai/requests-for-research/blob/master/_requests_for_research/im2latex.html) (convolutional encoder, recurrent decoder)\n"," * Generating [images by captions](https://arxiv.org/abs/1511.02793) (recurrent encoder, convolutional decoder)\n"," * Grapheme2phoneme - convert words to transcripts\n","  \n","We chose simplified __Hebrew->English__ machine translation for words and short phrases (character-level), as it is relatively quick to train even without a gpu cluster."]},{"cell_type":"code","metadata":{"id":"hdWZWHTW3qtw","executionInfo":{"status":"ok","timestamp":1628680609704,"user_tz":-540,"elapsed":417,"user":{"displayName":"퇴근요정","photoUrl":"","userId":"05102711429894151560"}}},"source":["import sys, os\n","if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n","\n","    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n","\n","    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/grading.py -O ../grading.py\n","    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week6_outro/seq2seq/submit.py\n","    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week6_outro/seq2seq/basic_model_torch.py\n","    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week6_outro/seq2seq/he-pron-wiktionary.txt\n","    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week6_outro/seq2seq/main_dataset.txt\n","    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week6_outro/seq2seq/voc.py\n","\n","    !touch .setup_complete"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"wCGLgD6S3qtx","executionInfo":{"status":"ok","timestamp":1628680610609,"user_tz":-540,"elapsed":4,"user":{"displayName":"퇴근요정","photoUrl":"","userId":"05102711429894151560"}}},"source":["# If True, only translates phrases shorter than 20 characters (way easier).\n","EASY_MODE = True\n","# Useful for initial coding.\n","# If false, works with all phrases (please switch to this mode for homework assignment)\n","\n","# way we translate. Either \"he-to-en\" or \"en-to-he\"\n","MODE = \"he-to-en\"\n","# maximal length of _generated_ output, does not affect training\n","MAX_OUTPUT_LENGTH = 50 if not EASY_MODE else 20\n","REPORT_FREQ = 100                          # how often to evaluate validation score"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_NihG_653qtx"},"source":["## Preprocessing\n","\n","We shall store dataset as a dictionary\n","`{ word1:[translation1,translation2,...], word2:[...],...}`.\n","\n","This is mostly due to the fact that many words have several correct translations.\n","\n","We have implemented this thing for you so that you can focus on more interesting parts."]},{"cell_type":"code","metadata":{"id":"xNGpNRG63qty","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628680611158,"user_tz":-540,"elapsed":552,"user":{"displayName":"퇴근요정","photoUrl":"","userId":"05102711429894151560"}},"outputId":"5043a613-ea06-47f6-a2b5-62beab5e8ab5"},"source":["import numpy as np\n","from collections import defaultdict\n","word_to_translation = defaultdict(list)  # our dictionary\n","\n","bos = '_'\n","eos = ';'\n","\n","with open(\"main_dataset.txt\", encoding=\"utf-8\") as fin:\n","    for line in fin:\n","\n","        en, he = line[:-1].lower().replace(bos, ' ').replace(eos,\n","                                                             ' ').split('\\t')\n","        word, trans = (he, en) if MODE == 'he-to-en' else (en, he)\n","\n","        if len(word) < 3:\n","            continue\n","        if EASY_MODE:\n","            if max(len(word), len(trans)) > 20:\n","                continue\n","\n","        word_to_translation[word].append(trans)\n","\n","print(\"size =\", len(word_to_translation))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["size = 130113\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mECE-3Pf3qty","executionInfo":{"status":"ok","timestamp":1628680611158,"user_tz":-540,"elapsed":3,"user":{"displayName":"퇴근요정","photoUrl":"","userId":"05102711429894151560"}}},"source":["# get all unique lines in source language\n","all_words = np.array(list(word_to_translation.keys()))\n","# get all unique lines in translation language\n","all_translations = np.array([ts for all_ts in word_to_translation.values() for ts in all_ts])"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7J1CDKVh3qtz"},"source":["### Split the dataset\n","\n","We hold out 10% of all words to be used for validation."]},{"cell_type":"code","metadata":{"id":"wbQcpm2s3qt0","executionInfo":{"status":"ok","timestamp":1628680611752,"user_tz":-540,"elapsed":596,"user":{"displayName":"퇴근요정","photoUrl":"","userId":"05102711429894151560"}}},"source":["from sklearn.model_selection import train_test_split\n","train_words, test_words = train_test_split(\n","    all_words, test_size=0.1, random_state=42)"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wLzy7z_A3qt1"},"source":["## Building vocabularies\n","\n","We now need to build vocabularies that map strings to token ids and vice versa. We're gonna need these fellas when we feed training data into model or convert output matrices into english words."]},{"cell_type":"code","metadata":{"id":"YudBTgtn3qt2","executionInfo":{"status":"ok","timestamp":1628680612552,"user_tz":-540,"elapsed":802,"user":{"displayName":"퇴근요정","photoUrl":"","userId":"05102711429894151560"}}},"source":["from voc import Vocab\n","inp_voc = Vocab.from_lines(''.join(all_words), bos=bos, eos=eos, sep='')\n","out_voc = Vocab.from_lines(''.join(all_translations), bos=bos, eos=eos, sep='')"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"J242yEhO3qt2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628680612552,"user_tz":-540,"elapsed":2,"user":{"displayName":"퇴근요정","photoUrl":"","userId":"05102711429894151560"}},"outputId":"e769908c-c795-4b01-c464-18fcc74fb0a0"},"source":["# Here's how you cast lines into ids and backwards.\n","batch_lines = all_words[:5]\n","batch_ids = inp_voc.to_matrix(batch_lines)\n","batch_lines_restored = inp_voc.to_lines(batch_ids)\n","\n","print(\"lines\")\n","print(batch_lines)\n","print(\"\\nwords to ids (0 = bos, 1 = eos):\")\n","print(batch_ids)\n","print(\"\\nback to words\")\n","print(batch_lines_restored)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["lines\n","['אנרכיזם' 'אוטיזם קלאסי' 'אלבדו' 'אלבמה' 'אכילס']\n","\n","words to ids (0 = bos, 1 = eos):\n","[[  0 113 129 137 124 122 119 126   1   1   1   1   1   1]\n"," [  0 113 118 121 122 119 126   2 136 125 113 130 122   1]\n"," [  0 113 125 114 116 118   1   1   1   1   1   1   1   1]\n"," [  0 113 125 114 127 117   1   1   1   1   1   1   1   1]\n"," [  0 113 124 122 125 130   1   1   1   1   1   1   1   1]]\n","\n","back to words\n","['אנרכיזם', 'אוטיזם קלאסי', 'אלבדו', 'אלבמה', 'אכילס']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_RCjgoSs3qt2"},"source":["Draw word/translation length distributions to estimate the scope of the task."]},{"cell_type":"code","metadata":{"id":"x2R0uWMr3qt3","colab":{"base_uri":"https://localhost:8080/","height":407},"executionInfo":{"status":"ok","timestamp":1628680613319,"user_tz":-540,"elapsed":768,"user":{"displayName":"퇴근요정","photoUrl":"","userId":"05102711429894151560"}},"outputId":"7be86796-4891-4eb5-dc32-7c3b4b61099f"},"source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","plt.figure(figsize=[8, 4])\n","plt.subplot(1, 2, 1)\n","plt.title(\"words\")\n","plt.hist(list(map(len, all_words)), bins=20)\n","\n","plt.subplot(1, 2, 2)\n","plt.title('translations')\n","plt.hist(list(map(len, all_translations)), bins=20)"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([   21.,   112.,  3098.,  8157., 11482., 12556., 11430.,  9568.,\n","         9254.,  9755., 10299., 11123., 11203., 10840.,  9316.,  7873.,\n","         6527.,  5523.,  4505.,  3684.]),\n"," array([ 1.  ,  1.95,  2.9 ,  3.85,  4.8 ,  5.75,  6.7 ,  7.65,  8.6 ,\n","         9.55, 10.5 , 11.45, 12.4 , 13.35, 14.3 , 15.25, 16.2 , 17.15,\n","        18.1 , 19.05, 20.  ]),\n"," <a list of 20 Patch objects>)"]},"metadata":{"tags":[]},"execution_count":9},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 576x288 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"OgDmHDKr3qt3"},"source":["## Deploy encoder-decoder\n","\n","__The assignment starts here__\n","\n","Our architecture consists of two main blocks:\n","* Encoder reads words character by character and outputs code vector (usually a function of last RNN state)\n","* Decoder takes that code vector and produces translations character by character\n","\n","Than it gets fed into a model that follows this simple interface:\n","* __`model(inp, out, **flags) -> logp`__ - takes symbolic int32 matrices of hebrew words and their english translations. Computes the log-probabilities of all possible english characters given english prefices and hebrew word.\n","* __`model.translate(inp, **flags) -> out, logp`__ - takes symbolic int32 matrix of hebrew words, produces output tokens sampled from the model and output log-probabilities for all possible tokens at each tick.\n","  * if given flag __`greedy=True`__, takes most likely next token at each iteration. Otherwise samples with next token probabilities predicted by model.\n","\n","That's all! It's as hard as it gets. With those two methods alone you can implement all kinds of prediction and training."]},{"cell_type":"code","metadata":{"id":"uLFRb7-w3qt3","executionInfo":{"status":"ok","timestamp":1628680617256,"user_tz":-540,"elapsed":3940,"user":{"displayName":"퇴근요정","photoUrl":"","userId":"05102711429894151560"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"pe7lWfZH3qt3","executionInfo":{"status":"ok","timestamp":1628680617259,"user_tz":-540,"elapsed":10,"user":{"displayName":"퇴근요정","photoUrl":"","userId":"05102711429894151560"}}},"source":["from basic_model_torch import BasicTranslationModel\n","model = BasicTranslationModel(inp_voc, out_voc,\n","                              emb_size=64, hid_size=256)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"A4rdNqHe3qt4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628680617259,"user_tz":-540,"elapsed":9,"user":{"displayName":"퇴근요정","photoUrl":"","userId":"05102711429894151560"}},"outputId":"fed5219d-b36d-4127-9f15-1d3b30064dcb"},"source":["# Play around with symbolic_translate and symbolic_score\n","inp = torch.tensor(np.random.randint(0, 10, [3, 5]), dtype=torch.int64)\n","out = torch.tensor(np.random.randint(0, 10, [3, 5]), dtype=torch.int64)\n","\n","# translate inp (with untrained model)\n","sampled_out, logp = model.translate(inp, greedy=False)\n","\n","print(\"Sample translations:\\n\", sampled_out)\n","print(\"Log-probabilities at each step:\\n\", logp)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Sample translations:\n"," tensor([[  0,  38, 188, 203, 265, 127,  26, 276, 268, 145,  41,  17, 138, 269,\n","         147,   2, 260, 227,  25,  71, 252, 161, 115, 178, 109,  36,  25,  85,\n","         234,  95,  89, 133,  43, 243, 160, 159, 220,  95,  36, 276,  46,   6,\n","         117, 127, 183,  25, 274,  27, 226,  18, 282,  72,  48, 171,  78, 197,\n","         219, 101, 228, 148, 164,  62, 194, 212, 251,  61, 123, 170, 154,  56,\n","          74,  31, 145, 101, 191, 207, 217, 268, 237,  97, 154, 157,   5, 273,\n","          91, 127, 115,  67, 173, 168,  84, 199, 242, 240,   6, 179,  68, 267,\n","         173,  14, 260, 266, 189, 162, 188, 224, 200, 229, 203,  67, 281, 112,\n","         120,  78, 213, 164,  71,  50, 235, 173,  41, 275, 163,  42, 229,  95,\n","          17, 248,  10, 269, 109, 142, 101, 255,   4, 214, 183, 214, 270, 147,\n","         139, 271, 281, 192,  10,  15, 171, 109,  75,  36, 242, 206, 103, 138,\n","         124,  38, 225, 224, 140,  95,  36, 201,  38, 217, 163,  45, 149, 279,\n","          61, 106, 120, 101,  96, 175,  51, 103,  56, 171, 231,  36, 275, 163,\n","          53, 101, 102,  22, 281, 235,  60, 110, 255, 229, 124,   0,  52, 245,\n","         205, 206, 217,  22, 249,  54, 109, 168, 282,   5, 147,  13,  38, 174,\n","         200,  41, 133,  50, 201,  79,  82,  86, 238,  60, 227,  72,   9, 219,\n","         273,  10,  54,  85, 241,  41, 195,   1, 247, 190,   3, 208, 122, 108,\n","         194,  14,  85, 153,  36,  90, 282,  54, 138, 120, 180, 274, 263, 187,\n","         261, 233,  91, 258, 110,  30,  38, 278, 171, 107, 278, 150,  90, 222,\n","          37, 168, 120, 190, 137, 125, 275, 209, 237,  93,  78, 231,  21, 171,\n","         276, 224, 231, 224,  22, 163, 107,  84, 159, 126,  30, 191, 167,  80,\n","          51],\n","        [  0, 171, 191,  36, 193,  37,  34, 166,  76,  71, 151,  41, 197, 265,\n","         223, 182, 248, 156, 176, 111,  79,  83,   5,  58,  99, 108, 182, 184,\n","          31, 196, 160,  44, 167, 205, 112, 204, 211, 144,  87, 231, 219, 173,\n","          73, 279, 215,  41, 187,  20,  82,  30,  40, 241,  52,  76, 234,  16,\n","         148, 224, 185, 134, 123, 260, 254, 175, 173, 235, 122, 199, 156,   7,\n","         104, 214, 103, 230, 210, 195,  52, 261,  26, 167, 225, 158,  40,   8,\n","         127,  23, 274, 185, 155, 157,  89, 197,  43, 194, 121, 240, 143, 106,\n","         159, 259, 257,  44, 249, 191, 234, 266, 178, 264, 120,  81,  13,  81,\n","         247,  93, 272, 172,  22, 186, 267, 268,  43,  59, 196, 149, 231, 114,\n","         262, 100, 261, 121, 102, 138,  53, 108,  33, 225,  99, 109, 163, 277,\n","         247, 112, 169, 184, 249, 274,  57, 238, 162, 263, 137, 249, 273, 217,\n","         190,   7, 278, 135, 140, 248,  65, 278, 225, 278,  35, 188, 264, 160,\n","         273, 223,  55, 243, 153, 186, 207, 152, 260, 204, 101, 260, 171, 168,\n","         213, 257,  97, 219,  60, 242, 167, 176, 195, 138,  11,  11,  60, 158,\n","         182, 200, 131, 271, 259,  95,  47, 226,  53, 258, 174, 142, 135,  46,\n","          79, 141,  74, 279,  42, 214, 159, 199, 114,  99,  63, 278, 255, 200,\n","          29,  33, 222,  29, 116, 250, 173, 102,  40, 175, 118,  27, 162,  33,\n","          15, 251, 252, 110,  88, 246, 278,  29, 264, 156, 240, 100,  65, 216,\n","         125, 146, 106,  85,  88,  73,  91,  90, 173,  83,  32, 245, 177,  53,\n","         234, 221, 281,  27, 182, 172,  44, 112, 178,  87, 275, 261, 248, 128,\n","         104, 117,  20,  62, 281,   5, 194,  74, 200,  72, 126, 211, 165,  65,\n","           1],\n","        [  0,   9, 206, 256, 198,  25, 211, 228,  36, 230, 282,  62, 235, 263,\n","          65, 159, 220, 203,  98,  43,  40,  55, 229, 251, 169, 262, 177, 274,\n","         158, 180,   7, 173, 122, 113,  83,  13, 104, 129, 145,  40, 223,  16,\n","          31,  69,  21, 128,  15, 242,  62, 194, 163,  65,  70,  83,  46, 234,\n","         276, 108, 246, 177, 130,  88, 159, 158, 110, 275,  29, 250, 124, 100,\n","         197, 116,  24, 234,  76, 172, 120, 231,   9, 168, 145,  67, 193, 232,\n","         102, 142, 218, 250, 134,  13,  14,  67, 262, 182,  26, 194,  38,  36,\n","         216,  16, 275, 168,  71,  80, 199, 156,  33,  72,   2, 144,  24, 223,\n","         129, 248, 112, 122, 269, 190, 173, 270, 210, 208,  55,  23,   6, 112,\n","          53, 264,  48, 166, 185, 280,  20,  43,  63,  63, 159,  25, 144, 182,\n","          13, 238, 167, 219, 260,  84,  61, 210, 228, 227, 251,  14,  20,  87,\n","          69,  61,  67,  73, 265, 269, 253, 279, 114, 149,  60, 220,  95,  27,\n","           2, 195,   8, 238,  90, 116, 183, 181, 236,  95, 103, 105, 223,  55,\n","         226, 132, 124, 132, 238, 226, 251, 174,  83, 125, 102,  50, 141, 186,\n","         148, 204,  86, 190,  29,  98,  17,  44, 123,  95, 157, 271, 281,  26,\n","          27, 259, 228, 119, 100, 258, 188, 197,  11, 155, 166, 176,  91, 231,\n","         172, 142,   1, 166,  41, 173,  86, 138,   4, 110,  83, 233, 127, 135,\n","         111, 230, 208,   1, 149, 204, 164, 174,  37, 188, 106, 171, 258, 106,\n","          71,  12, 261, 219,  35, 237, 149,  77,   5, 250, 185,  97,   8, 181,\n","           9,  90,  16,  87, 122,  31, 149,  51, 158, 185,  31,  44,  43, 243,\n","         100, 259,  67, 256, 176, 276,  71, 185, 193,  63, 190, 189, 247, 221,\n","         115]])\n","Log-probabilities at each step:\n"," tensor([[[  0.0000, -69.0776, -69.0776,  ..., -69.0776, -69.0776, -69.0776],\n","         [ -5.7692,  -5.5417,  -5.5104,  ...,  -5.6196,  -5.6314,  -5.5313],\n","         [ -5.8326,  -5.4988,  -5.6058,  ...,  -5.6695,  -5.6171,  -5.7091],\n","         ...,\n","         [ -5.6880,  -5.6215,  -5.6147,  ...,  -5.7204,  -5.6676,  -5.6446],\n","         [ -5.6872,  -5.5768,  -5.6667,  ...,  -5.8301,  -5.6984,  -5.6458],\n","         [ -5.7669,  -5.5474,  -5.5253,  ...,  -5.7782,  -5.6966,  -5.7668]],\n","\n","        [[  0.0000, -69.0776, -69.0776,  ..., -69.0776, -69.0776, -69.0776],\n","         [ -5.6663,  -5.5136,  -5.5391,  ...,  -5.6648,  -5.5693,  -5.4883],\n","         [ -5.7567,  -5.6248,  -5.4666,  ...,  -5.7842,  -5.5845,  -5.5348],\n","         ...,\n","         [ -5.8772,  -5.6079,  -5.4973,  ...,  -5.5889,  -5.5769,  -5.7715],\n","         [ -5.7764,  -5.6699,  -5.5319,  ...,  -5.4531,  -5.6692,  -5.6236],\n","         [ -5.7655,  -5.6399,  -5.4575,  ...,  -5.5549,  -5.4862,  -5.5435]],\n","\n","        [[  0.0000, -69.0776, -69.0776,  ..., -69.0776, -69.0776, -69.0776],\n","         [ -5.7147,  -5.4906,  -5.5565,  ...,  -5.6412,  -5.6343,  -5.5544],\n","         [ -5.8536,  -5.6250,  -5.5305,  ...,  -5.6478,  -5.6288,  -5.4929],\n","         ...,\n","         [ -5.6292,  -5.7285,  -5.6232,  ...,  -5.5363,  -5.6704,  -5.5414],\n","         [ -5.6606,  -5.5141,  -5.5801,  ...,  -5.5736,  -5.6665,  -5.6718],\n","         [ -5.5783,  -5.6551,  -5.6958,  ...,  -5.6543,  -5.6872,  -5.6319]]],\n","       grad_fn=<LogSoftmaxBackward>)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RAC2V0cV3qt4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628680618389,"user_tz":-540,"elapsed":1137,"user":{"displayName":"퇴근요정","photoUrl":"","userId":"05102711429894151560"}},"outputId":"efa9bb2a-9cae-44b0-b979-0e20570ede66"},"source":["# score logp(out | inp) with untrained input\n","logp = model(inp, out)\n","print(\"Symbolic_score output:\\n\", logp)\n","\n","print(\"Log-probabilities of output tokens:\\n\",\n","      torch.gather(logp, dim=2, index=out[:, :, None]))"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Symbolic_score output:\n"," tensor([[[  0.0000, -69.0776, -69.0776,  ..., -69.0776, -69.0776, -69.0776],\n","         [ -5.8491,  -5.6601,  -5.6266,  ...,  -5.5426,  -5.7387,  -5.7471],\n","         [ -5.8040,  -5.5976,  -5.5752,  ...,  -5.6240,  -5.7771,  -5.6210],\n","         [ -5.7718,  -5.5877,  -5.6389,  ...,  -5.6427,  -5.6614,  -5.6456],\n","         [ -5.7640,  -5.5606,  -5.6017,  ...,  -5.6924,  -5.7343,  -5.6151]],\n","\n","        [[  0.0000, -69.0776, -69.0776,  ..., -69.0776, -69.0776, -69.0776],\n","         [ -5.7057,  -5.6469,  -5.6507,  ...,  -5.6733,  -5.7102,  -5.5471],\n","         [ -5.6627,  -5.7428,  -5.7014,  ...,  -5.6293,  -5.6371,  -5.6113],\n","         [ -5.6267,  -5.5754,  -5.6515,  ...,  -5.5861,  -5.7917,  -5.7254],\n","         [ -5.6536,  -5.5082,  -5.5444,  ...,  -5.6311,  -5.6785,  -5.6049]],\n","\n","        [[  0.0000, -69.0776, -69.0776,  ..., -69.0776, -69.0776, -69.0776],\n","         [ -5.7196,  -5.5598,  -5.5867,  ...,  -5.5688,  -5.6763,  -5.6152],\n","         [ -5.6437,  -5.4743,  -5.6135,  ...,  -5.5749,  -5.8318,  -5.7137],\n","         [ -5.6657,  -5.4678,  -5.5308,  ...,  -5.6371,  -5.7021,  -5.6033],\n","         [ -5.6297,  -5.6628,  -5.6370,  ...,  -5.6429,  -5.6493,  -5.6406]]],\n","       grad_fn=<LogSoftmaxBackward>)\n","Log-probabilities of output tokens:\n"," tensor([[[-69.0776],\n","         [ -5.6601],\n","         [ -5.7018],\n","         [ -5.5877],\n","         [ -5.7640]],\n","\n","        [[-69.0776],\n","         [ -5.7509],\n","         [ -5.7853],\n","         [ -5.6267],\n","         [ -5.6536]],\n","\n","        [[-69.0776],\n","         [ -5.6749],\n","         [ -5.6437],\n","         [ -5.5809],\n","         [ -5.7722]]], grad_fn=<GatherBackward>)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vO9FdnvC3qt4","executionInfo":{"status":"ok","timestamp":1628680988007,"user_tz":-540,"elapsed":413,"user":{"displayName":"퇴근요정","photoUrl":"","userId":"05102711429894151560"}}},"source":["def translate(lines, max_len=MAX_OUTPUT_LENGTH):\n","    \"\"\"\n","    You are given a list of input lines. \n","    Make your neural network translate them.\n","    :return: a list of output lines\n","    \"\"\"\n","    # Convert lines to a matrix of indices\n","    lines_ix = inp_voc.to_matrix(lines)\n","    lines_ix = torch.tensor(lines_ix, dtype=torch.int64)\n","\n","    # Compute translations in form of indices\n","    trans_ix, _ = model.translate(lines_ix, greedy=True)\n","\n","    # Convert translations back into strings\n","    return out_voc.to_lines(trans_ix.data.numpy())"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"8ifyBPBe3qt4","colab":{"base_uri":"https://localhost:8080/","height":394},"executionInfo":{"status":"error","timestamp":1628681320897,"user_tz":-540,"elapsed":269620,"user":{"displayName":"퇴근요정","photoUrl":"","userId":"05102711429894151560"}},"outputId":"9338d113-e9b2-4864-ec50-3bda71bd198e"},"source":["print(\"Sample inputs:\", all_words[:3])\n","print(\"Dummy translations:\", translate(all_words[:3]))\n","trans = translate(all_words[:3])\n","\n","assert translate(all_words[:3]) == translate(\n","    all_words[:3]), \"make sure translation is deterministic (use greedy=True and disable any noise layers)\"\n","assert type(translate(all_words[:3])) is list and (type(translate(all_words[:1])[0]) is str or type(\n","    translate(all_words[:1])[0]) is unicode), \"translate(lines) must return a sequence of strings!\"\n","# note: if translation freezes, make sure you used max_len parameter\n","print(\"Tests passed!\")"],"execution_count":26,"outputs":[{"output_type":"stream","text":["Sample inputs: ['אנרכיזם' 'אוטיזם קלאסי' 'אלבדו']\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-19ffd1d1feda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sample inputs:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dummy translations:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m assert translate(all_words[:3]) == translate(\n","\u001b[0;32m<ipython-input-25-ed18b59cb709>\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(lines, max_len)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Compute translations in form of indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtrans_ix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines_ix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgreedy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Convert translations back into strings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/basic_model_torch.py\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(self, inp, greedy, max_len, eps, **flags)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mhid_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0mhid_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhid_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgreedy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/basic_model_torch.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, prev_state, prev_tokens, **flags)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mprev_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memb_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mnew_dec_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdec0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_dec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0moutput_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_dec_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1133\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_ih\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_hh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_ih\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_hh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         )\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"fNmxoKYA3qt4"},"source":["### Scoring function\n","\n","LogLikelihood is a poor estimator of model performance.\n","* If we predict zero probability once, it shouldn't ruin entire model.\n","* It is enough to learn just one translation if there are several correct ones.\n","* What matters is how many mistakes model's gonna make when it translates!\n","\n","Therefore, we will use minimal Levenshtein distance. It measures how many characters do we need to add/remove/replace from model translation to make it perfect. Alternatively, one could use character-level BLEU/RougeL or other similar metrics.\n","\n","The catch here is that Levenshtein distance is not differentiable: it isn't even continuous. We can't train our neural network to maximize it by gradient descent."]},{"cell_type":"code","metadata":{"id":"GVE1Fyd23qt4","executionInfo":{"status":"ok","timestamp":1628681329363,"user_tz":-540,"elapsed":534,"user":{"displayName":"퇴근요정","photoUrl":"","userId":"05102711429894151560"}}},"source":["import editdistance  # !pip install editdistance\n","\n","\n","def get_distance(word, trans):\n","    \"\"\"\n","    A function that takes word and predicted translation\n","    and evaluates (Levenshtein's) edit distance to closest correct translation\n","    \"\"\"\n","    references = word_to_translation[word]\n","    assert len(references) != 0, \"wrong/unknown word\"\n","    return min(editdistance.eval(trans, ref) for ref in references)\n","\n","\n","def score(words, bsize=100):\n","    \"\"\"a function that computes levenshtein distance for bsize random samples\"\"\"\n","    assert isinstance(words, np.ndarray)\n","\n","    batch_words = np.random.choice(words, size=bsize, replace=False)\n","    batch_trans = translate(batch_words)\n","\n","    distances = list(map(get_distance, batch_words, batch_trans))\n","\n","    return np.array(distances, dtype='float32')"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"OK3BNfUR3qt5"},"source":["# should be around 5-50 and decrease rapidly after training :)\n","[score(test_words, 10).mean() for _ in range(5)]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6CXGobiM3qt5"},"source":["## Supervised pre-training\n","\n","Here we define a function that trains our model through maximizing log-likelihood a.k.a. minimizing crossentropy."]},{"cell_type":"code","metadata":{"id":"calQeP3R3qt5","executionInfo":{"status":"aborted","timestamp":1628680618391,"user_tz":-540,"elapsed":9,"user":{"displayName":"퇴근요정","photoUrl":"","userId":"05102711429894151560"}}},"source":["import random\n","\n","\n","def sample_batch(words, word_to_translation, batch_size):\n","    \"\"\"\n","    sample random batch of words and random correct translation for each word\n","    example usage:\n","    batch_x,batch_y = sample_batch(train_words, word_to_translations,10)\n","    \"\"\"\n","    # choose words\n","    batch_words = np.random.choice(words, size=batch_size)\n","\n","    # choose translations\n","    batch_trans_candidates = list(map(word_to_translation.get, batch_words))\n","    batch_trans = list(map(random.choice, batch_trans_candidates))\n","    return batch_words, batch_trans"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kMT-qXZg3qt5","executionInfo":{"status":"aborted","timestamp":1628680618391,"user_tz":-540,"elapsed":9,"user":{"displayName":"퇴근요정","photoUrl":"","userId":"05102711429894151560"}}},"source":["bx, by = sample_batch(train_words, word_to_translation, batch_size=3)\n","print(\"Source:\")\n","print(bx)\n","print(\"Target:\")\n","print(by)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bsik7Rsh3qt5","executionInfo":{"status":"aborted","timestamp":1628680618391,"user_tz":-540,"elapsed":9,"user":{"displayName":"퇴근요정","photoUrl":"","userId":"05102711429894151560"}}},"source":["from basic_model_torch import infer_length, infer_mask, to_one_hot\n","\n","\n","def compute_loss_on_batch(input_sequence, reference_answers):\n","    \"\"\" Compute crossentropy loss given a batch of sources and translations \"\"\"\n","    input_sequence = torch.tensor(inp_voc.to_matrix(input_sequence), dtype=torch.int64)\n","    reference_answers = torch.tensor(out_voc.to_matrix(reference_answers), dtype=torch.int64)\n","\n","    # Compute log-probabilities of all possible tokens at each step. Use model interface.\n","    logprobs_seq = <YOUR CODE>\n","\n","    # compute elementwise crossentropy as negative log-probabilities of reference_answers.\n","    crossentropy = - \\\n","        torch.sum(logprobs_seq *\n","                  to_one_hot(reference_answers, len(out_voc)), dim=-1)\n","    assert crossentropy.dim(\n","    ) == 2, \"please return elementwise crossentropy, don't compute mean just yet\"\n","\n","    # average with mask\n","    mask = infer_mask(reference_answers, out_voc.eos_ix)\n","    loss = torch.sum(crossentropy * mask) / torch.sum(mask)\n","\n","    return loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jrPT4uEG3qt6","executionInfo":{"status":"aborted","timestamp":1628680618392,"user_tz":-540,"elapsed":10,"user":{"displayName":"퇴근요정","photoUrl":"","userId":"05102711429894151560"}}},"source":["# test it\n","loss = compute_loss_on_batch(*sample_batch(train_words, word_to_translation, 3))\n","print('loss = ', loss)\n","\n","assert loss.item() > 0.0\n","loss.backward()\n","for w in model.parameters():\n","    assert w.grad is not None and torch.max(torch.abs(w.grad)).item() != 0, \\\n","        \"Loss is not differentiable w.r.t. a weight with shape %s. Check comput_loss_on_batch.\" % (\n","            w.size(),)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i73M5XQJ3qt6"},"source":["### Actually train the model\n","\n","Minibatches and stuff..."]},{"cell_type":"code","metadata":{"id":"WrTL0G573qt6","executionInfo":{"status":"aborted","timestamp":1628680618392,"user_tz":-540,"elapsed":10,"user":{"displayName":"퇴근요정","photoUrl":"","userId":"05102711429894151560"}}},"source":["from IPython.display import clear_output\n","from tqdm.notebook import tqdm, trange\n","loss_history = []\n","editdist_history = []\n","entropy_history = []\n","opt = torch.optim.Adam(model.parameters())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NDhA1RMM3qt6","executionInfo":{"status":"aborted","timestamp":1628680618392,"user_tz":-540,"elapsed":9,"user":{"displayName":"퇴근요정","photoUrl":"","userId":"05102711429894151560"}}},"source":["for i in trange(25000):\n","    loss = compute_loss_on_batch(*sample_batch(train_words, word_to_translation, 32))\n","\n","    # train with backprop\n","    loss.backward()\n","    opt.step()\n","    opt.zero_grad()\n","\n","    loss_history.append(loss.item())\n","\n","    if (i+1) % REPORT_FREQ == 0:\n","        clear_output(True)\n","        current_scores = score(test_words)\n","        editdist_history.append(current_scores.mean())\n","        print(\"llh=%.3f, mean score=%.3f\" %\n","              (np.mean(loss_history[-10:]), np.mean(editdist_history[-10:])))\n","        plt.figure(figsize=(12, 4))\n","        plt.subplot(131)\n","        plt.title('train loss / traning time')\n","        plt.plot(loss_history)\n","        plt.grid()\n","        plt.subplot(132)\n","        plt.title('val score distribution')\n","        plt.hist(current_scores, bins=20)\n","        plt.subplot(133)\n","        plt.title('val score / traning time (lower is better)')\n","        plt.plot(editdist_history)\n","        plt.grid()\n","        plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ILhXqfET3qt6"},"source":["__How to interpret the plots:__\n","\n","* __Train loss__ - that's your model's crossentropy over minibatches. It should go down steadily. Most importantly, it shouldn't be NaN :)\n","* __Val score distribution__ - distribution of translation edit distance (score) within batch. It should move to the left over time.\n","* __Val score / training time__ - it's your current mean edit distance. This plot is much whimsier than loss, but make sure it goes below 8 by 2500 steps. \n","\n","If it doesn't, first try to re-create both model and opt. You may have changed it's weight too much while debugging. If that doesn't help, it's debugging time."]},{"cell_type":"code","metadata":{"id":"85vkAfXh3qt6","executionInfo":{"status":"aborted","timestamp":1628680618392,"user_tz":-540,"elapsed":9,"user":{"displayName":"퇴근요정","photoUrl":"","userId":"05102711429894151560"}}},"source":["for word in train_words[:10]:\n","    print(\"%s -> %s\" % (word, translate([word])[0]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TOIGrKLf3qt6","executionInfo":{"status":"aborted","timestamp":1628680618392,"user_tz":-540,"elapsed":9,"user":{"displayName":"퇴근요정","photoUrl":"","userId":"05102711429894151560"}}},"source":["test_scores = []\n","for start_i in trange(0, len(test_words), 32):\n","    batch_words = test_words[start_i:start_i+32]\n","    batch_trans = translate(batch_words)\n","    distances = list(map(get_distance, batch_words, batch_trans))\n","    test_scores.extend(distances)\n","\n","print(\"Supervised test score:\", np.mean(test_scores))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N7yZ-_LX3qt7"},"source":["## Self-critical policy gradient\n","\n","In this section you'll implement algorithm called self-critical sequence training (here's an [article](https://arxiv.org/abs/1612.00563)).\n","\n","The algorithm is a vanilla policy gradient with a special baseline. \n","\n","$$ \\nabla J = E_{x \\sim p(s)} E_{y \\sim \\pi(y|x)} \\nabla log \\pi(y|x) \\cdot (R(x,y) - b(x)) $$\n","\n","Here reward R(x,y) is a __negative levenshtein distance__ (since we minimize it). The baseline __b(x)__ represents how well model fares on word __x__.\n","\n","In practice, this means that we compute baseline as a score of greedy translation, $b(x) = R(x,y_{greedy}(x)) $.\n","\n","![img](https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/scheme.png)\n","\n","\n","Luckily, we already obtained the required outputs: `model.greedy_translations, model.greedy_mask` and we only need to compute levenshtein using `compute_levenshtein` function."]},{"cell_type":"code","metadata":{"id":"omtstIug3qt7","executionInfo":{"status":"aborted","timestamp":1628680618393,"user_tz":-540,"elapsed":10,"user":{"displayName":"퇴근요정","photoUrl":"","userId":"05102711429894151560"}}},"source":["def compute_reward(input_sequence, translations):\n","    \"\"\" computes sample-wise reward given token ids for inputs and translations \"\"\"\n","    distances = list(map(get_distance,\n","                         inp_voc.to_lines(input_sequence.data.numpy()),\n","                         out_voc.to_lines(translations.data.numpy())))\n","    # use negative levenshtein distance so that larger reward means better policy\n","    return - torch.tensor(distances, dtype=torch.int64)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2rvm0Cou3qt7","executionInfo":{"status":"aborted","timestamp":1628680618393,"user_tz":-540,"elapsed":10,"user":{"displayName":"퇴근요정","photoUrl":"","userId":"05102711429894151560"}}},"source":["def scst_objective_on_batch(input_sequence, max_len=MAX_OUTPUT_LENGTH):\n","    \"\"\" Compute pseudo-loss for policy gradient given a batch of sources \"\"\"\n","    input_sequence = torch.tensor(inp_voc.to_matrix(input_sequence), dtype=torch.int64)\n","\n","    # use model to __sample__ symbolic translations given input_sequence\n","    sample_translations, sample_logp = <YOUR CODE>\n","    # use model to __greedy__ symbolic translations given input_sequence\n","    greedy_translations, greedy_logp = <YOUR CODE>\n","\n","    # compute rewards and advantage\n","    rewards = compute_reward(input_sequence, sample_translations)\n","    baseline = <YOUR CODE: compute __negative__ levenshtein for greedy mode>\n","\n","    # compute advantage using rewards and baseline\n","    advantage =  <YOUR CODE>\n","\n","    # compute log_pi(a_t|s_t), shape = [batch, seq_length]\n","    logp_sample = <YOUR CODE>\n","    \n","    # ^-- hint: look at how crossentropy is implemented in supervised learning loss above\n","    # mind the sign - this one should not be multiplied by -1 :)\n","\n","    # policy gradient pseudo-loss. Gradient of J is exactly policy gradient.\n","    J = logp_sample * advantage[:, None]\n","\n","    assert J.dim() == 2, \"please return elementwise objective, don't compute mean just yet\"\n","\n","    # average with mask\n","    mask = infer_mask(sample_translations, out_voc.eos_ix)\n","    loss = - torch.sum(J * mask) / torch.sum(mask)\n","\n","    # regularize with negative entropy. Don't forget the sign!\n","    # note: for entropy you need probabilities for all tokens (sample_logp), not just logp_sample\n","    entropy = <YOUR CODE: compute entropy matrix of shape[batch, seq_length], H = -sum(p*log_p), don't forget the sign!>\n","    # hint: you can get sample probabilities from sample_logp using math :)\n","\n","    assert entropy.dim(\n","    ) == 2, \"please make sure elementwise entropy is of shape [batch,time]\"\n","\n","    reg = - 0.01 * torch.sum(entropy * mask) / torch.sum(mask)\n","\n","    return loss + reg, torch.sum(entropy * mask) / torch.sum(mask)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J8i8Gbt33qt7"},"source":["## Policy gradient training"]},{"cell_type":"code","metadata":{"id":"0hcxD_7L3qt7","executionInfo":{"status":"aborted","timestamp":1628680618393,"user_tz":-540,"elapsed":10,"user":{"displayName":"퇴근요정","photoUrl":"","userId":"05102711429894151560"}}},"source":["entropy_history = [np.nan] * len(loss_history)\n","opt = torch.optim.Adam(model.parameters(), lr=1e-5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MIaBH6wG3qt7","executionInfo":{"status":"aborted","timestamp":1628680618393,"user_tz":-540,"elapsed":10,"user":{"displayName":"퇴근요정","photoUrl":"","userId":"05102711429894151560"}}},"source":["for i in trange(100000):\n","    loss, ent = scst_objective_on_batch(\n","        sample_batch(train_words, word_to_translation, 32)[0])  # [0] = only source sentence\n","\n","    # train with backprop\n","    loss.backward()\n","    opt.step()\n","    opt.zero_grad()\n","\n","    loss_history.append(loss.item())\n","    entropy_history.append(ent.item())\n","\n","    if (i+1) % REPORT_FREQ == 0:\n","        clear_output(True)\n","        current_scores = score(test_words)\n","        editdist_history.append(current_scores.mean())\n","        plt.figure(figsize=(12, 4))\n","        plt.subplot(131)\n","        plt.title('val score distribution')\n","        plt.hist(current_scores, bins=20)\n","        plt.subplot(132)\n","        plt.title('val score / traning time')\n","        plt.plot(editdist_history)\n","        plt.grid()\n","        plt.subplot(133)\n","        plt.title('policy entropy / traning time')\n","        plt.plot(entropy_history)\n","        plt.grid()\n","        plt.show()\n","        print(\"J=%.3f, mean score=%.3f\" %\n","              (np.mean(loss_history[-10:]), np.mean(editdist_history[-10:])))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b0BE0PKJ3qt7"},"source":["__Debugging tips:__\n","<img src=https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/do_something_scst.png width=400>\n","\n"," * As usual, don't expect improvements right away, but in general the model should be able to show some positive changes by 5k steps.\n"," * Entropy is a good indicator of many problems. \n","   * If it reaches zero, you may need greater entropy regularizer.\n","   * If it has rapid changes time to time, you may need gradient clipping.\n","   * If it oscillates up and down in an erratic manner... it's perfectly okay for entropy to do so. But it should decrease at the end.\n","   \n"," * We don't show loss_history cuz it's uninformative for pseudo-losses in policy gradient. However, if something goes wrong you can check it to see if everything isn't a constant zero."]},{"cell_type":"markdown","metadata":{"id":"zZG3BLj93qt8"},"source":["## Results"]},{"cell_type":"code","metadata":{"id":"HemzYIei3qt8","executionInfo":{"status":"aborted","timestamp":1628680618394,"user_tz":-540,"elapsed":11,"user":{"displayName":"퇴근요정","photoUrl":"","userId":"05102711429894151560"}}},"source":["for word in train_words[:10]:\n","    print(\"%s -> %s\" % (word, translate([word])[0]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r5-P_kRS3qt8","executionInfo":{"status":"aborted","timestamp":1628680618394,"user_tz":-540,"elapsed":11,"user":{"displayName":"퇴근요정","photoUrl":"","userId":"05102711429894151560"}}},"source":["test_scores = []\n","for start_i in trange(0, len(test_words), 32):\n","    batch_words = test_words[start_i:start_i+32]\n","    batch_trans = translate(batch_words)\n","    distances = list(map(get_distance, batch_words, batch_trans))\n","    test_scores.extend(distances)\n","print(\"Policy gradient test score:\", np.mean(test_scores))\n","\n","# ^^ If you get Out Of MemoryError, please replace this with batched computation"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"86w4uqPe3qt8"},"source":["## Submit to Coursera"]},{"cell_type":"code","metadata":{"id":"NPRUz-Rp3qt8","executionInfo":{"status":"aborted","timestamp":1628680618394,"user_tz":-540,"elapsed":11,"user":{"displayName":"퇴근요정","photoUrl":"","userId":"05102711429894151560"}}},"source":["from submit import submit_seq2seq\n","\n","submit_seq2seq(test_scores, 'your.email@example.com', 'YourAssignmentToken')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DM3F2tj13qt8"},"source":["## Optional: Make it actually work\n","\n","In this section we want you to finally __restart with EASY_MODE=False__ and experiment to find a good model/curriculum for that task.\n","\n","We recommend you to start with the following architecture\n","\n","```\n","encoder---decoder\n","\n","           P(y|h)\n","             ^\n"," LSTM  ->   LSTM\n","  ^          ^\n"," biLSTM  ->   LSTM\n","  ^          ^\n","input       y_prev\n","```\n","\n","__Note:__ you can fit all 4 state tensors of both LSTMs into a in a single state - just assume that it contains, for example, [h0, c0, h1, c1] - pack it in encode and update in decode.\n","\n","\n","Here are some cool ideas on what you can do then.\n","\n","__General tips & tricks:__\n","* You will likely need to adjust pre-training time for such a network.\n","* Supervised pre-training may benefit from clipping gradients somehow.\n","* SCST may indulge a higher learning rate in some cases and changing entropy regularizer over time.\n","* It's often useful to save pre-trained model parameters to not re-train it every time you want new policy gradient parameters. \n","* When leaving training for nighttime, try setting REPORT_FREQ to a larger value (e.g. 500) not to waste time on it.\n","\n","### Attention\n","There's more than one way to connect decoder to encoder\n","  * __Vanilla:__ layer_i of encoder last state goes to layer_i of decoder initial state\n","  * __Every tick:__ feed encoder last state _on every iteration_ of decoder.\n","  * __Attention:__ allow decoder to \"peek\" at one (or several) positions of encoded sequence on every tick.\n","  \n","The most effective (and cool) of those is, of course, attention.\n","You can read more about attention [in this nice blog post](https://distill.pub/2016/augmented-rnns/). The easiest way to begin is to use \"soft\" attention with \"additive\" or \"dot-product\" intermediate layers.\n","\n","__Tips__\n","* Model usually generalizes better if you no longer allow decoder to see final encoder state\n","* Once your model made it through several epochs, it is a good idea to visualize attention maps to understand what your model has actually learned\n","\n","* There's more stuff [here](https://github.com/yandexdataschool/Practical_RL/blob/master/week8_scst/bonus.ipynb)\n","* If you opted for hard attention, we recommend [gumbel-softmax](https://blog.evjang.com/2016/11/tutorial-categorical-variational.html) instead of sampling. Also please make sure soft attention works fine before you switch to hard.\n","\n","### UREX\n","* This is a way to improve exploration in policy-based settings. The main idea is that you find and upweight under-appreciated actions.\n","* Here's [video](https://www.youtube.com/watch?v=fZNyHoXgV7M&feature=youtu.be&t=3444)\n"," and an [article](https://arxiv.org/abs/1611.09321).\n","* You may want to reduce batch size 'cuz UREX requires you to sample multiple times per source sentence.\n","* Once you got it working, try using experience replay with importance sampling instead of (in addition to) basic UREX.\n","\n","### Some additional ideas:\n","* (advanced deep learning) It may be a good idea to first train on small phrases and then adapt to larger ones (a.k.a. training curriculum).\n","* (advanced nlp) You may want to switch from raw utf8 to something like unicode or even syllables to make task easier.\n","* (advanced nlp) Since hebrew words are written __with vowels omitted__, you may want to use a small Hebrew vowel markup dataset at `he-pron-wiktionary.txt`.\n","\n"]},{"cell_type":"code","metadata":{"id":"B_REZCp03qt8","executionInfo":{"status":"aborted","timestamp":1628680618394,"user_tz":-540,"elapsed":11,"user":{"displayName":"퇴근요정","photoUrl":"","userId":"05102711429894151560"}}},"source":["assert not EASY_MODE, \"make sure you set EASY_MODE = False at the top of the notebook.\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G_DnOBkq3qt8"},"source":["__Contributions:__ This notebook is brought to you by\n","* Yandex [MT team](https://tech.yandex.com/translate/)\n","* Denis Mazur ([DeniskaMazur](https://github.com/DeniskaMazur)), Oleg Vasilev ([Omrigan](https://github.com/Omrigan/)), Dmitry Emelyanenko ([TixFeniks](https://github.com/tixfeniks)) and Fedor Ratnikov ([justheuristic](https://github.com/justheuristic/))\n","* Dataset is parsed from [Wiktionary](https://en.wiktionary.org), which is under CC-BY-SA and GFDL licenses.\n"]}]}