{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"TPU","colab":{"name":"5_1_practice_reinforce_pytorch.ipynb","provenance":[{"file_id":"https://github.com/yandexdataschool/Practical_RL/blob/coursera/week5_policy_based/practice_reinforce_pytorch.ipynb","timestamp":1627199267817}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"EI-uielhXOZD"},"source":["# REINFORCE in PyTorch\n","\n","Just like we did before for Q-learning, this time we'll design a PyTorch network to learn `CartPole-v0` via policy gradient (REINFORCE).\n","\n","Most of the code in this notebook is taken from approximate Q-learning, so you'll find it more or less familiar and even simpler."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6xxrdbgsXOZF","executionInfo":{"status":"ok","timestamp":1627214761130,"user_tz":-540,"elapsed":32712,"user":{"displayName":"퇴근요정","photoUrl":"","userId":"05102711429894151560"}},"outputId":"a7aa7f08-5cd8-43b4-f8c0-0d4cc71aebee"},"source":["import sys, os\n","if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n","    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n","\n","    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/grading.py -O ../grading.py\n","    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week5_policy_based/submit.py\n","\n","    !touch .setup_complete\n","\n","# This code creates a virtual display to draw game images on.\n","# It will have no effect if your machine has a monitor.\n","if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n","    !bash ../xvfb start\n","    os.environ['DISPLAY'] = ':1'"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Selecting previously unselected package xvfb.\n","(Reading database ... 160837 files and directories currently installed.)\n","Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.9_amd64.deb ...\n","Unpacking xvfb (2:1.19.6-1ubuntu4.9) ...\n","Setting up xvfb (2:1.19.6-1ubuntu4.9) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Starting virtual X frame buffer: Xvfb.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"B3i7mQGcXOZF","executionInfo":{"status":"ok","timestamp":1627214762249,"user_tz":-540,"elapsed":1121,"user":{"displayName":"퇴근요정","photoUrl":"","userId":"05102711429894151560"}}},"source":["import gym\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wsI3PZT9XOZG"},"source":["A caveat: with some versions of `pyglet`, the following cell may crash with `NameError: name 'base' is not defined`. The corresponding bug report is [here](https://github.com/pyglet/pyglet/issues/134). If you see this error, try restarting the kernel."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":287},"id":"14VKdrspXOZG","executionInfo":{"status":"ok","timestamp":1627214764008,"user_tz":-540,"elapsed":1761,"user":{"displayName":"퇴근요정","photoUrl":"","userId":"05102711429894151560"}},"outputId":"3fdadb17-89cd-4123-df8a-e3cea411c97b"},"source":["env = gym.make(\"CartPole-v0\")\n","\n","# gym compatibility: unwrap TimeLimit\n","if hasattr(env, '_max_episode_steps'):\n","    env = env.env\n","\n","env.reset()\n","n_actions = env.action_space.n\n","state_dim = env.observation_space.shape\n","\n","plt.imshow(env.render(\"rgb_array\"))"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7f924dd6afd0>"]},"metadata":{"tags":[]},"execution_count":3},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"cBAZ7rCyXOZG"},"source":["# Building the network for REINFORCE"]},{"cell_type":"markdown","metadata":{"id":"QmeIVmkKXOZG"},"source":["For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n","\n","For numerical stability, please __do not include the softmax layer into your network architecture__.\n","We'll use softmax or log-softmax where appropriate."]},{"cell_type":"code","metadata":{"id":"sz3A-MJiXOZH","executionInfo":{"status":"ok","timestamp":1627214770396,"user_tz":-540,"elapsed":4295,"user":{"displayName":"퇴근요정","photoUrl":"","userId":"05102711429894151560"}}},"source":["import torch\n","import torch.nn as nn"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"a76R4psDXOZH","executionInfo":{"status":"ok","timestamp":1627214861126,"user_tz":-540,"elapsed":299,"user":{"displayName":"퇴근요정","photoUrl":"","userId":"05102711429894151560"}}},"source":["# Build a simple neural network that predicts policy logits. \n","# Keep it simple: CartPole isn't worth deep architectures.\n","model = nn.Sequential(\n","    nn.Linear(state_dim[0], 32),\n","    nn.ReLU(),\n","    nn.Linear(32, 64),\n","    nn.ReLU(),\n","    nn.Linear(64, 32),\n","    nn.ReLU(),\n","    nn.Linear(32, n_actions),\n","    nn.ReLU()\n",")"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ds2KHx2oXOZI"},"source":["#### Predict function"]},{"cell_type":"markdown","metadata":{"id":"QlAxgzDIXOZI"},"source":["Note: output value of this function is not a torch tensor, it's a numpy array.\n","So, here gradient calculation is not needed.\n","<br>\n","Use [no_grad](https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad)\n","to suppress gradient calculation.\n","<br>\n","Also, `.detach()` (or legacy `.data` property) can be used instead, but there is a difference:\n","<br>\n","With `.detach()` computational graph is built but then disconnected from a particular tensor,\n","so `.detach()` should be used if that graph is needed for backprop via some other (not detached) tensor;\n","<br>\n","In contrast, no graph is built by any operation in `no_grad()` context, thus it's preferable here."]},{"cell_type":"code","metadata":{"id":"7g6A68vuXOZI","executionInfo":{"status":"ok","timestamp":1627214770397,"user_tz":-540,"elapsed":3,"user":{"displayName":"퇴근요정","photoUrl":"","userId":"05102711429894151560"}}},"source":["def predict_probs(states):\n","    \"\"\" \n","    Predict action probabilities given states.\n","    :param states: numpy array of shape [batch, state_shape]\n","    :returns: numpy array of shape [batch, n_actions]\n","    \"\"\"\n","    # convert states, compute logits, use softmax to get probability\n","    with torch.no_grad():\n","        x = torch.tensor(states, dtype=torch.float32)\n","        y = model(x)\n","        y = nn.functional.softmax(y, -1)\n","    return y.detach().numpy()"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"PZslXou-XOZJ","executionInfo":{"status":"ok","timestamp":1627214770687,"user_tz":-540,"elapsed":293,"user":{"displayName":"퇴근요정","photoUrl":"","userId":"05102711429894151560"}}},"source":["test_states = np.array([env.reset() for _ in range(5)])\n","test_probas = predict_probs(test_states)\n","assert isinstance(test_probas, np.ndarray), \\\n","    \"you must return np array and not %s\" % type(test_probas)\n","assert tuple(test_probas.shape) == (test_states.shape[0], env.action_space.n), \\\n","    \"wrong output shape: %s\" % np.shape(test_probas)\n","assert np.allclose(np.sum(test_probas, axis=1), 1), \"probabilities do not sum to 1\""],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"twsJriLpXOZK"},"source":["### Play the game\n","\n","We can now use our newly built agent to play the game."]},{"cell_type":"code","metadata":{"id":"SxxSmK8PXOZK","executionInfo":{"status":"ok","timestamp":1627214926203,"user_tz":-540,"elapsed":447,"user":{"displayName":"퇴근요정","photoUrl":"","userId":"05102711429894151560"}}},"source":["def generate_session(env, t_max=1000):\n","    \"\"\" \n","    Play a full session with REINFORCE agent.\n","    Returns sequences of states, actions, and rewards.\n","    \"\"\"\n","    # arrays to record session\n","    states, actions, rewards = [], [], []\n","    s = env.reset()\n","\n","    for t in range(t_max):\n","        # action probabilities array aka pi(a|s)\n","        action_probs = predict_probs(np.array([s]))[0]\n","\n","        # Sample action with given probabilities.\n","        a = np.random.choice(n_actions, p=action_probs)\n","        new_s, r, done, info = env.step(a)\n","\n","        # record session history to train later\n","        states.append(s)\n","        actions.append(a)\n","        rewards.append(r)\n","\n","        s = new_s\n","        if done:\n","            break\n","\n","    return states, actions, rewards"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"ur4i0RroXOZK","executionInfo":{"status":"ok","timestamp":1627214773966,"user_tz":-540,"elapsed":294,"user":{"displayName":"퇴근요정","photoUrl":"","userId":"05102711429894151560"}}},"source":["# test it\n","states, actions, rewards = generate_session(env)"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZbFnfWGdXOZL"},"source":["### Computing cumulative rewards\n","\n","$$\n","\\begin{align*}\n","G_t &= r_t + \\gamma r_{t + 1} + \\gamma^2 r_{t + 2} + \\ldots \\\\\n","&= \\sum_{i = t}^T \\gamma^{i - t} r_i \\\\\n","&= r_t + \\gamma * G_{t + 1}\n","\\end{align*}\n","$$"]},{"cell_type":"code","metadata":{"id":"L4Vk1QPOXOZL","executionInfo":{"status":"ok","timestamp":1627214775526,"user_tz":-540,"elapsed":3,"user":{"displayName":"퇴근요정","photoUrl":"","userId":"05102711429894151560"}}},"source":["def get_cumulative_rewards(rewards,  # rewards at each step\n","                           gamma=0.99  # discount for reward\n","                           ):\n","    \"\"\"\n","    Take a list of immediate rewards r(s,a) for the whole session \n","    and compute cumulative returns (a.k.a. G(s,a) in Sutton '16).\n","    \n","    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n","\n","    A simple way to compute cumulative rewards is to iterate from the last\n","    to the first timestep and compute G_t = r_t + gamma*G_{t+1} recurrently\n","\n","    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n","    \"\"\"\n","    cum_rewards = []\n","    for i, r in enumerate(reversed(rewards)):\n","        if i == 0:\n","            cum_rewards.append(r)\n","        else:\n","            cum_rewards.append(r + gamma * cum_rewards[i - 1])\n","    return list(reversed(cum_rewards))"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GsNErKImXOZL","executionInfo":{"status":"ok","timestamp":1627214777147,"user_tz":-540,"elapsed":8,"user":{"displayName":"퇴근요정","photoUrl":"","userId":"05102711429894151560"}},"outputId":"21253b7d-a715-4367-fe47-aa471753a699"},"source":["get_cumulative_rewards(rewards)\n","assert len(get_cumulative_rewards(list(range(100)))) == 100\n","assert np.allclose(\n","    get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n","    [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n","assert np.allclose(\n","    get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n","    [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n","assert np.allclose(\n","    get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0),\n","    [0, 0, 1, 2, 3, 4, 0])\n","print(\"looks good!\")"],"execution_count":11,"outputs":[{"output_type":"stream","text":["looks good!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"c7zTEwvVXOZL"},"source":["#### Loss function and updates\n","\n","We now need to define objective and update over policy gradient.\n","\n","Our objective function is\n","\n","$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n","\n","REINFORCE defines a way to compute the gradient of the expected reward with respect to policy parameters. The formula is as follows:\n","\n","$$ \\nabla_\\theta \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\nabla_\\theta \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n","\n","We can abuse PyTorch's capabilities for automatic differentiation by defining our objective function as follows:\n","\n","$$ \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n","\n","When you compute the gradient of that function with respect to network weights $\\theta$, it will become exactly the policy gradient."]},{"cell_type":"code","metadata":{"id":"xKxsh4tKXOZL","executionInfo":{"status":"ok","timestamp":1627214780939,"user_tz":-540,"elapsed":286,"user":{"displayName":"퇴근요정","photoUrl":"","userId":"05102711429894151560"}}},"source":["def to_one_hot(y_tensor, ndims):\n","    \"\"\" helper: take an integer vector and convert it to 1-hot matrix. \"\"\"\n","    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n","    y_one_hot = torch.zeros(\n","        y_tensor.size()[0], ndims).scatter_(1, y_tensor, 1)\n","    return y_one_hot"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"PrilVAWcXOZM","executionInfo":{"status":"ok","timestamp":1627214948174,"user_tz":-540,"elapsed":299,"user":{"displayName":"퇴근요정","photoUrl":"","userId":"05102711429894151560"}}},"source":["# Your code: define optimizers\n","optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n","\n","\n","def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n","    \"\"\"\n","    Takes a sequence of states, actions and rewards produced by generate_session.\n","    Updates agent's weights by following the policy gradient above.\n","    Please use Adam optimizer with default parameters.\n","    \"\"\"\n","\n","    # cast everything into torch tensors\n","    states = torch.tensor(states, dtype=torch.float32, requires_grad=True)\n","    actions = torch.tensor(actions, dtype=torch.int32)\n","    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n","    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32, requires_grad=True)\n","\n","    # predict logits, probas and log-probas using an agent.\n","    logits = model(states)\n","    probs = nn.functional.softmax(logits, -1)\n","    log_probs = nn.functional.log_softmax(logits, -1)\n","\n","    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n","        \"please use compute using torch tensors and don't use predict_probs function\"\n","\n","    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n","    log_probs_for_actions = torch.sum(\n","        log_probs * to_one_hot(actions, env.action_space.n), dim=1)\n","   \n","    # Compute loss here. Don't forget entropy regularization with `entropy_coef` \n","    entropy = - torch.mean(torch.sum(probs * log_probs, dim=1))\n","    J = - torch.mean(log_probs_for_actions * cumulative_returns)\n","    loss = J + entropy_coef * entropy\n","\n","    # Gradient descent step\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    # technical: return session rewards to print them later\n","    return np.sum(rewards)"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VfUst2brXOZM"},"source":["### The actual training"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xjmZPscWXOZM","executionInfo":{"status":"ok","timestamp":1627214977603,"user_tz":-540,"elapsed":27673,"user":{"displayName":"퇴근요정","photoUrl":"","userId":"05102711429894151560"}},"outputId":"da9f0190-44de-483b-8be0-149bae86974c"},"source":["for i in range(100):\n","    rewards = [train_on_session(*generate_session(env)) for _ in range(100)]  # generate new sessions\n","    \n","    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n","    \n","    if np.mean(rewards) > 300:\n","        print(\"You Win!\")  # but you can train even further\n","        break"],"execution_count":19,"outputs":[{"output_type":"stream","text":["mean reward:24.850\n","mean reward:33.370\n","mean reward:39.750\n","mean reward:72.000\n","mean reward:116.420\n","mean reward:189.790\n","mean reward:208.140\n","mean reward:242.800\n","mean reward:344.820\n","You Win!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OQz1A8ZUXOZM"},"source":["### Results & video"]},{"cell_type":"code","metadata":{"id":"L7vJ3aXjXOZM"},"source":["# Record sessions\n","\n","import gym.wrappers\n","\n","with gym.wrappers.Monitor(gym.make(\"CartPole-v0\"), directory=\"videos\", force=True) as env_monitor:\n","    sessions = [generate_session(env_monitor) for _ in range(100)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a5WJ3cVsXOZM"},"source":["# Show video. This may not work in some setups. If it doesn't\n","# work for you, you can download the videos and view them locally.\n","\n","from pathlib import Path\n","from IPython.display import HTML\n","\n","video_names = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n","\n","HTML(\"\"\"\n","<video width=\"640\" height=\"480\" controls>\n","  <source src=\"{}\" type=\"video/mp4\">\n","</video>\n","\"\"\".format(video_names[-1]))  # You can also try other indices"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2BBF_1tqXOZN","executionInfo":{"status":"ok","timestamp":1627214997488,"user_tz":-540,"elapsed":5585,"user":{"displayName":"퇴근요정","photoUrl":"","userId":"05102711429894151560"}},"outputId":"850cfdb7-a72e-4bbe-ea49-3fa45dc2a703"},"source":["from submit import submit_cartpole\n","submit_cartpole(generate_session, 'bds0822@gmail.com', 'MjnIcgaHvwOUhDvs')"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Your average reward is 245.92 over 100 episodes\n","Submitted to Coursera platform. See results on assignment page!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ElGYptDSXOZN"},"source":["That's all, thank you for your attention!\n","\n","Not having enough? There's an actor-critic waiting for you in the honor section. But make sure you've seen the videos first."]}]}